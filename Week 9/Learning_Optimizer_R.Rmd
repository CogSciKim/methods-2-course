---
title: "Economic games"
author: "Sigurd Sørensen"
date: "3/31/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Class 8 Different Optimizers and regression approahces. 
We've been introduced to normal lm() linear regression function. But linear regression isn't just linear regression. There exist many different approaches and criteria for which that algorithm should optimize. Ordinary least squares (OLS) which you now know how to do algebraically using matrix multiplication. The least squares method tries to minimize error term $\epsilon_i = y_i - (a+b x_i) $. We cannot work with the error term directly since this would require us to know the true estimates of a and b. However we know the estimates of a and b which we can dentote as $\hat{a}$. The goal is therefore to minimize the residuals   $r_i = y_i -(\hat{a} + \hat{b}x_i)$. More precisely the residual sum of squares (RSS). Calculating the OLS algebraically can be very computaionally expensive so linear regression OLS algorithms usually use other tricks to find the $\theta$ that minimize RSS, remember $\theta$ is the symbol for all our parameter values (Beta-values among others). So we can also frame OLS as $\underset{\theta}{\arg\min f(x)}$. In a machine learning framework we would call the RSS = f(x) our cost-function or loss-function. We wanna minimize our cost/loss when doing regression.  

But how would we implement such an optimizer in R?
Well this among other things is what we're gonna learn today.
  - How to use the optim() function for finding maximums and minimums of a function. 
  - How to use the optim() function to find the $\theta$ for OLS.
  - How to create your own custom made cost function for regression.
  


## Optim() function
The optim() function in R is a minimization tool. It's goal is to the find x in a given function which minimizes f(x).

### Polynomial()
Let us start out by showing an example of on polynomial function. 

```{r}
# We first define our function, in our case it is a polynomial expression. 
polynomial <- function(x){
  f_x <- 10*x^2 + 2*x + 30 
  return(f_x)
}
x_plot <- seq(-10,10, by = .05)

#plot the function as a line (type = "l"). 
plot(x_plot, polynomial(x_plot), type = "l")
```
We that there is a minimum around $x \approx 0$.

**Question:**
- Using what we know about derivatives how would find any local maximum and minimum points?
Extrema are localized by finding the roots of the derivative

- When having found an extremum how can you be sure it is a min or max? (think higher order derivatives.) 
Acceleration (the double derivative) should be positive in the case of a minimum or negative in the case of a maximum.

After having answered let us look at how we can do this operation in R. 

```{r}
#check out ?optim()
?optim()
```
We can see that the function takes the following arguments, optim(par = initial values, fn = function to minimize, method = "Which algorithm to use", lower = "lower bound", upper = "upper bound".)

```{r}
#How to find the local minimum using Optim function()
#Without derivative method
optim(1, fn = polynomial, method= "Brent", lower = -10, upper = 10)
```
We get several different outputs. \$par indicates the x values which minimized f(x), and \$value indicates what the f(x) values is at the point x which minimized f(x).

```{r}
# with derivative
optim(1, fn = polynomial, method = "CG")
```
We can see that using an algorithm which uses the derivatives is slightly different in its estimates of x which argmin f(x). While it isn't as precise and is more of an estimate it has the advantage of not requiring a lower and upper boundary. 


### Sinus function. 
We know that a sinus function has several minimums and maximums so how does the optim() function deal with that? We will follow the same procedure as before, first define the function then try and optimize it. 
```{r}
sin_function <- function(x){
  f_x <- sin(x)
  return(f_x)
}
x <- seq(-10,10 , by = .05)
plot(x,sin_function(x), type = "l")
```
In the definition set of $d_f = (-10:10)$ we can see that there is roughly 3 local minimum. *1)* around x= 5, *2)* around x= -2, *3)* around x= -8. 

```{r}
#Optim will always find the local minimum which is the closest to the starting value.  
x = -10
repeat{
  print(paste("for x = ", x," the closest minimum is at x =",optim(x, sin_function, method = "CG")$par))
  x = x +1
  if (x > 10) break("X limit reached")
}
```

```{r}
f <- function(x1,y1){
 f_x <- (1-x1)^2 + 100*(y1- x1^2)^2
 return(f_x)
}
x <- seq(-2,2,by=.15)
y <- seq(-1,3,by=.15) 
z <- outer(x,y,f) #All possible combination of x,y is used to calculate all possible f(x,y) = z. 

#how to plot 3D
persp(x,y,z,phi=45,theta=-45,col="yellow",shade=.00000001,ticktype="detailed")

```
Now the question is how to we find local minimums in 3D space? Well we know if there is to be no functional growth then our $\nabla f(x,y) = \vec{0}$. Check out this https://tutorial.math.lamar.edu/classes/calciii/relativeextrema.aspx for a deeper understanding. For a introduction to Hesse-Matrix and the ABC formula for finding saddle-points minimums and maximum check out https://najamogeltoft.medium.com/finding-the-minima-maxima-and-saddle-point-s-of-multivariable-functions-4ac4a547f22f 


**Optim time.**
When using optim for multidimensional optimization, the input in your function definition must be a single vector.

```{r}
f <- function(x){
  f_x <- (1-x[1])^2 + 100*(x[2]-x[1]^2)^2
  return(f_x)
} 

optim(c(0,0) , f)
```
The closest minimum to (x = 0, y= 0) is (x = 1, y = 1). Can we be sure that is the global minimum? Not as it currently stands, we could modify our algorithm to look broader or do some weighted search but this is one of the big issues with optimizers.   

However, we are now quite close to a scenario in which we can optimize the RSS of simple regression model with an intercept and 1 predictor. Imagine that the X-axis is the different Beta_1 values and Y-axis is the different intercepts and the Z-axis is our cost/loss. We wanna find the intercept and slope or in other words the X and Y values which minimizes our RSS or Z-axis. All we need to do is create a function which calculates RSS based on our $\theta , X ,y$.

I'll show you an example:
```{r}
set.seed(101) #random seed to reproduce results
n <- 1e2
x <- rnorm(n, 20, 2) # so this is 1e2x1 predictor matrix 
y <- rnorm(n, mean = x *2, sd  =1 )                   # this is our outcome variable, a 1e2x1 vector
X_matrix <- cbind(rep(1, 100), x)      # adding a column of 1s as intercept to the design matrix X (1e2 x 1)
theta <- rep(0,2)               # set up the initial theta 2 x 1 vector
```


```{r}
loss_function <-function(X, y, par){  
  n <- length(y)
  loss <- sum((X%*%par - y)^2)/(n-length(par))
return(loss) 
}
```

```{r}
coef <- optim(par = theta, fn = loss_function , X = X_matrix, y = y, method = "BFGS")
coef$par
```
We now have the two point estimates of our intercept: -1.1967 and slope: 2.057. But we know from methods 1 that there is uncertainty denoted as the SE surrounding these coefficients.

```{r}


SE_beta_calc <- function(X,y,theta){
  n <- length(y)
  x <- X[,2]
  y_hat <- X %*% theta
  
  SE_beta <- ((1/(n-2)) *  (sum((y - y_hat)^2)) /  sum((x - mean(x))^2))
  return(SE_beta)
}

SE_beta_calc(X_matrix, y , coef$par)
```

Using lm() we can see that it doesn't quite yield the same SE for our coefficients this is because it uses a stricter estimation. 
```{r}
summary(lm(y~ x))
```
Now it is time for you to get your hands dirty with the optim() function. 

## Exercises

1) Choose a mathematical function with 4 minimums in the definition set you have chosen. 
  a) Hard code the function into R and plot it.
```{r}
library(tidyverse)

f <- function(x) sin(x)
df <- data.frame(x=c(0:8*pi))

ggplot(df)+aes(x)+geom_function(fun=f)+theme_bw()

```
  
  b) Find the 4 minimums using the optim() function. 
```{r}

x=2*pi
minlist = c()

repeat{
  minimum = optim(x, f,method="CG")$par
  print(paste("for x = ", x," the closest minimum is at x =",minimum))
  x = x + 2*pi
  minlist = append(minlist,minimum)
  if (x > 8*pi) break("X limit reached")
}


```
  
  c) Check if the they are indeed minimums using the second derivative rule we learned last class. 
```{r}
cos(minlist)
-sin(minlist)

```
  
  d) Find the maximums or in other words, find the x's which maximizes f(x). (Hint: Optim() always minimizes the return() so maybe switch the sign? How can max become min? )
```{r}
reverse_f = function(x) -sin(x)

x=0

repeat{
  maximum = optim(x, reverse_f,method="CG")$par
  print(paste("for x = ", x," the closest maximum is at x =",maximum))
  x = x + 2*pi
  minlist = append(minlist,minimum)
  if (x > 6*pi) break("X limit reached")
}
```


  
2) Using the above introduction to the linear regression using optim().
  a) Create Nx5 design matrix with the intercept and 4 different predictors. 
```{r}

x <- seq(1,10)
design <- matrix(c(rep(1,10),x,x^2,sqrt(x),exp(x)),nrow=10)

```

  b) Simulate y depended on the design matrix. (Hint: Make y dependend on all the different predictors.) don't forget to add some error. 
```{r}
beta <- c(16,13,1,2,1e-4)

true_val <- design%*%beta + rnorm(10,0,10)
```

  c) Create a loss function which we want to minimize (I would suggest RSS or MSE to start with.) 
```{r}
loss <- function(true_y,x,beta){
  sum((true_y-x%*%beta)^2)
}

```

  d) Use optim() to find the beta coefficients which minimizes our cost function. 
```{r}
optim_pars <- optim(rep(0,5),loss,x=design,true_y=true_val)
coeff <- optim_pars$par

optim_pars
```

```{r}
design%*%coeff
design%*%beta
```


3) It is time to start getting creative. 
  a) Create a design matrix with one predictor and simulate y from that design matrix.
```{r}
design <- matrix(c(rep(1,10),seq(1,10)),ncol=2)
beta <- c(1,2)

y <- design%*%beta+rnorm(10,0,0.5)
```
  
  b) Make at least 3-5 cost functions which you think could make sense. 
```{r}
loss1 <- function(x,y,param){sum((y-x%*%param)^2)} # RSS

loss2 <- function(x,y,param){sqrt((sum(y-x%*%param)^2)/(length(y)-length(param)))} # RSE

loss3 <- function(x,y,param){sum(abs(y-x%*%param))/length(y)} # MAE

```
  
  c) Use optim() on the data from exercise 3a using your newly made cost functions. 
```{r}
optim(rep(0,2),loss1,x=design,y=y)$par
optim(rep(0,2),loss2,x=design,y=y)$par
optim(rep(0,2),loss3,x=design,y=y)$par

```
  
  d) Try and plot the cost. (Hint: you need to simulate the intercept values and slope values to find the cost function which then can be plotted)
```{r}
p1 <- seq(0,2,0.1)
p2 <- seq(0,2,0.1)

reslist1 <- c()
reslist2 <- c()
reslist3 <- c()

for (i in 1:(length(p1)*length(p2))){
  params <- as.numeric(expand.grid(p1,p2)[i,])
  reslist1 <- append(reslist1,loss1(x=design,y=y,params))
  reslist2 <- append(reslist2,loss2(x=design,y=y,params))
  reslist3 <- append(reslist3,loss3(x=design,y=y,params))
} 

library(plotly)

plot_ly(x=expand.grid(p1,p2)$Var1,y=expand.grid(p1,p2)$Var2,z=reslist1)

plot_ly(x=expand.grid(p1,p2)$Var1,y=expand.grid(p1,p2)$Var2,z=reslist2)

plot_ly(x=expand.grid(p1,p2)$Var1,y=expand.grid(p1,p2)$Var2,z=reslist3)

```



### Exercises from the book. 
**8.1**
a) Nominal
b) Ordinal
c) Ordinal
d) Ratio
e) Nominal
f) Nominal
g) Ordinal/nominal
h) Nominal

**8.2** 
a) Interval

b) Ordinal

c) Nominal

d)
```{r}
annoying_df <- data.frame(Grade=c(37,39,28,73,50,59,41,57,46,41,62,28,26,66,53,54,37,46,25)) %>% 
  mutate(nominal=if_else(Grade > 60 | Grade < 45,"Atypical","Typical"),ordinal=ifelse(Grade < 45,"Low",ifelse(Grade > 60,"High","Medium")))

mean(annoying_df$Grade)
sd(annoying_df$Grade)

```


**8.3** 
```{r}
df <- data.frame(vacancy=c(0,1,2,3,4),y_count = c(59,27,9,1,0))

df$y_expected <- dpois(c(0,1,2,3,4),0.5)*96

df

```


**8.4**
Interval, ratio is implied

**(extra)**

#### Maximum Likelhood estimation
using log-likelihood
```{r}
log_likelihood_function <- function(model, y){
  y_hat = fitted.values(model)
  temp_vec = y * log(p) + (1-y) * log(1-p)
  return (sum(temp_vec))
}
```


## Regression to the mean. 
Since we have a slope coefficient below between 0-1 we have an instance of regression towards the mean. 

A kid who's mother were 10 cm above average will them self be on average 10 * 0.79 = 7.9 cm higher than the average daughter. Since the daughter regressed towards the mean she is now more average in height.

That daughter one day grows up to be a pretty woman gets married to rich good looking gent and they suddenly want kids. If the model holds the daughter of the daughter would then regress even more towards the mean since the daughter was 7.9 cm higher than average her daughter will then be 7.9 * 0.79 = 6.241 cm higher than the average daughter. 

This would continue for generation and generation if weren't for the error term.


We will now simulate this scenario with no error term. Watch close attention to the y-axis as we got from generation to generation. 
```{r}
#How many samples
n <- 1e2
slope <- 0.43
#Simulate mothers height. 
height_of_mothers <- rnorm(n, mean = 170, sd = 7.5)
height_of_mothers_central <- height_of_mothers - mean(height_of_mothers)

#Slope of 0.76
height_of_daughters <- 170 + height_of_mothers_central * slope
height_of_daughters_central <- height_of_daughters - mean(height_of_daughters)

#run regression model
lm_height <- lm(height_of_daughters ~ height_of_mothers_central)
summary(lm_height)


plot( y = height_of_daughters,  x= height_of_mothers_central)
abline(a = lm_height$coefficients[1], b= lm_height$coefficients[2])
```
```{r}
height_daughter_of_daughter <-170 + height_of_daughters_central * slope
height_daughter_of_daughter_central <- height_daughter_of_daughter - mean(height_daughter_of_daughter)

#plot
plot(height_of_daughters_central, height_daughter_of_daughter)


```
```{r}
height_daughter_of_daughter_of_daughter <- 170 + slope * height_daughter_of_daughter_central


plot(height_daughter_of_daughter_central , height_daughter_of_daughter_of_daughter)
```
We can see that the slope slowly regresses towards the mean and if we repeated this enough times it would approximate a flat line. 

However we know that we're not getting lower and lower and getting regressing towards the mean. Let's try and add some uncertainity. 

```{r}
#How many samples
n <- 1e2
slope <- 0.43
#Simulate mothers height. 
height_of_mothers <- rnorm(n, mean = 170, sd = 7.5)
height_of_mothers_central <- height_of_mothers - mean(height_of_mothers)

#Slope of 0.76
height_of_daughters <- rnorm(n, mean = 170 + height_of_mothers_central * slope, sd = 7.5)
height_of_daughters_central <- height_of_daughters - mean(height_of_daughters)

#run regression model
lm_height <- lm(height_of_daughters ~ height_of_mothers_central)
summary(lm_height)


plot( y = height_of_daughters,  x= height_of_mothers_central)
abline(a = lm_height$coefficients[1], b= lm_height$coefficients[2])
```
```{r}
height_daughter_of_daughter <- rnorm(n, mean = 170 + height_of_daughters_central * slope, sd = 7.5)
height_daughter_of_daughter_central <- height_daughter_of_daughter - mean(height_daughter_of_daughter)

#plot
plot(height_of_daughters_central, height_daughter_of_daughter)


```
```{r}
height_daughter_of_daughter_of_daughter <- rnorm(n , mean = 170 + slope * height_daughter_of_daughter_central, sd = 7.5)

plot(height_daughter_of_daughter_central , height_daughter_of_daughter_of_daughter)
```
Adding uncertainty counteracts that which would otherwise regress towards the mean.

### Your Turn:
**Exercise 1**
Which of the following statements would you choose to describe a beta coefficient for  Daughter_height ~ Mother_height(Centralized).
      1) *The beta coefficient for mothers_height of* $\beta = 0.79 , SE = 0.10 , p < 0.05$ *shows the effect of mothers_height on daughter being 0.79 with a SE = 0.10.*
      2) *The beta coefficient for mothers_height of* $\beta = 0.79 , SE = 0.10 , p < 0.05$
      *shows that a mother one cm taller compared to another mother is suspected to have a daughter who is 0.79 cm higher with 95% confidence intervals of [0.59, 0.99].* 

Number 2 is most descriptive.

**Exercise 2**
Come up with your own example of regression towards the mean: 
  a) Simulate your own example with repeated dependent iterations.
      1) With no error
      2) With error
```{r}
step1 <- rnorm(100, mean = 100, sd = 50)
step1_standard <- step1 - mean(step1)
```

```{r}
step2 <- 100 + step1_standard * 0.1
step2_standard <- step2 - mean(step2)

step3 <- 100 + step2_standard * 0.1
step3_standard <- step3 - mean(step2)

step4 <- 100 + step3_standard * 0.1

```

```{r}
step2_err <- rnorm(100, mean = 100 + step1_standard * 0.1, sd = 10)
step2_err_standard <- step1 - mean(step1)

step3_err <- rnorm(100, mean = 100 + step2_err_standard * 0.1, sd = 10)
step3_err_standard <- step1 - mean(step1)

step4_err <- rnorm(100, mean = 100 + step3_err_standard * 0.1, sd = 10)


```


  b) Check each step with a linear regression and a plot.
```{r}
df <- data.frame(step1,step1_standard,step2,step2_standard,step3,step3_standard)

df %>% ggplot()+aes(x=step1_standard,y=step2)+geom_point()+geom_smooth(method="lm",formula=y~x)
df %>% ggplot()+aes(x=step2_standard,y=step3)+geom_point()+geom_smooth(method="lm",formula=y~x)
df %>% ggplot()+aes(x=step3_standard,y=step4)+geom_point()+geom_smooth(method="lm",formula=y~x)

```
```{r}
df_err <- data.frame(step1,step1_standard,step2_err,step2_err_standard,step3_err,step3_err_standard)

df_err %>% ggplot()+aes(x=step1_standard,y=step2_err)+geom_point()+geom_smooth(method="lm",formula=y~x)
df_err %>% ggplot()+aes(x=step2_err_standard,y=step3_err)+geom_point()+geom_smooth(method="lm",formula=y~x)
df_err %>% ggplot()+aes(x=step3_err_standard,y=step4_err)+geom_point()+geom_smooth(method="lm",formula=y~x)

```



### Exercises from the book. 
**6.2** Programming fake-data simulation: Write an R function to: (i) simulate n data points from the model, y = a + bx + error, with data points x uniformly sampled from the range (0, 100) and with errors drawn independently from the normal distribution with mean 0 and standard deviation σ; (ii) fit a linear regression to the simulated data; and (iii) make a scatter plot of the data and fitted regression line. Your function should take as arguments, a, b, n, σ, and it should return the data, print out the fitted regression, and make the plot. Check your function by trying it out on some values of a, b, n, σ.
```{r}
sim_data <- function(a,b,n,sigma){
  x <- runif(n,0,100)
  y <- a+b*x+rnorm(n,0,sigma)
  print(summary(lm(y~x)))
  print(ggplot(data.frame(x,y))+aes(x,y)+geom_point()+geom_smooth(method="lm"))
  
  return(y)
}

sim_data(0,1,10,10)
sim_data(0,1,10,1000)
sim_data(300,-1,100,10)

```


**6.3** Variation, uncertainty, and sample size: Repeat the example in Section 6.2, varying the number of data points, n. What happens to the parameter estimates and uncertainties when you increase the number of observations?
```{r}
sim_data(10,1,10,10)
sim_data(10,1,20,10)
sim_data(10,1,30,10)
sim_data(10,1,40,10)
sim_data(10,1,100,10)
sim_data(10,1,1000,10)
sim_data(10,1,10000,10)
```
Estimate precision increases.

**6.4** Simulation study: Perform the previous exercise more systematically, trying out a sequence of values of n, for each simulating fake data and fitting the regression to obtain estimate and uncertainty (median and mad sd) for each parameter. Then plot each of these as a function of n and report on what you find.
```{r}
sim_data <- function(n,a=1,b=0,sigma=10){
  x <- runif(n,0,100)
  y <- a+b*x+rnorm(n,0,sigma)
  coeff <- lm(y~x)
  result <- as.data.frame(summary(coeff)$coefficients) %>% 
  select("Estimate","Std. Error") %>% 
  mutate(row = row_number(),n) %>%
  pivot_wider(names_from=row,values_from = c('Estimate','Std. Error'))
  
  return(result)
}

df <- sim_data(10)

for (i in seq(100,5000,100)){
  df <- rbind(df,sim_data(i))
}
df
names(df) <- c("n","est","int","est_se","int_se")

df %>% ggplot()+aes(x=n,est)+geom_smooth()
df %>% ggplot()+aes(x=n,y=est_se)+geom_smooth()
df %>% ggplot()+aes(x=n,int)+geom_smooth()
df %>% ggplot()+aes(x=n,y=int_se)+geom_smooth()

```
SE drops and converges, estimates converge around true values





